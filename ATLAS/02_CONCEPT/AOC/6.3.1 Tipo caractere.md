---
tags:
  - learning
  - oldVoult
HUB:
  - "[[hub-aoc]]"
---
### 6.3.1 Tipo caractere

A representação de informações em um computador é feita por meio de uma correspondência entre símbolos e grupos de bits binários. Cada símbolo, como um caractere, número ou símbolo, é identificado por um código específico que corresponde a um padrão de bits.

“A” ⇒ Algarismos binários “10101101”

Você pode se perguntar como é possível representar, com apenas dois sím- é possível representar, com apenas dois sím- representar, com apenas dois símbolos (0 e 1), todos os caracteres alfabéticos, algarismos decimais, sinais de pontuação, de operações matemáticas, entre outros, necessários à elaboração de um programa de computador. Monteiro (2007) afirma que a resposta para essa pergunta seria: pela utilização do método chamado de codificação, pelo qual cada símbolo da nossa linguagem tem um correspondente grupo de bits que identifica univocamente o referido símbolo (caractere).

Existem alguns padrões de codificação previamente definidos, conforme apresentados no Quadro 6.1




A utilização de padrões de codificação (ex.: ASCII, Unicode) é o método primário de introdução de informações no computador. As demais formas de representação de informação (tipos de dados) surgem no decorrer do processo de compilação ou interpretação do programa. O padrão de codificação mais utilizado pela indústria de computadores é o ASCII. A codificação correspondente a esse padrão já é parte do hardware (armazenado em uma memória do tipo ROM) e é definida pelo próprio fabricante. A Figura 6.2 apresenta parte da tabela ASCII como exemplo.